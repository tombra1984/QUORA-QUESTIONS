{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Tombra\\train.csv\\train - Copy.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319512, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download stopwords and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Convert non-string columns to strings\n",
    "df['question1'] = df['question1'].astype(str)\n",
    "df['question2'] = df['question2'].astype(str)\n",
    "\n",
    "# Combine two text columns into a single column\n",
    "df['combined_text'] = df['question1'] + df['question2']\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stopwords cleaning and punctuation removal\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Normalization (optional, can be skipped depending on the use case)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing function to the combined text column\n",
    "df['preprocessed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Example usage: Create a bag-of-words representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['preprocessed_text'])\n",
    "\n",
    "# X now contains the vectorized representation of the preprocessed text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text data\n",
    "sentences = [nltk.word_tokenize(text) for text in df['combined_text']]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(r\"C:\\Users\\Tombra\\word2vec\\model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['combined_text']\n",
    "labels = df['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Calculate TF-IDF\n",
    "# Set the parameters\n",
    "max_features = 2500\n",
    "min_df = 7\n",
    "max_df = 0.8\n",
    "stop_words = nltk_stopwords.words('english')  # Use nltk_stopwords instead of stopwords\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer to the features data\n",
    "tfidf_vectorizer.fit(features)\n",
    "\n",
    "# Transform the features using the fitted vectorizer\n",
    "vectorized_features = tfidf_vectorizer.transform(features)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model = gensim.models.Word2Vec.load(r\"C:\\Users\\Tombra\\word2vec\\model.bin\")\n",
    "\n",
    "def calculate_word2vec_similarity(text1, text2):\n",
    "    tokens1 = nltk.word_tokenize(text1)\n",
    "    tokens2 = nltk.word_tokenize(text2)\n",
    "    # Remove stopwords and punctuation\n",
    "    stopwords = set(nltk_stopwords.words('english'))\n",
    "    tokens1 = [token.lower() for token in tokens1 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    tokens2 = [token.lower() for token in tokens2 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Check if any of the lists is empty\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate similarity using Word2Vec model\n",
    "    similarity = model.wv.n_similarity(tokens1, tokens2)\n",
    "    return similarity\n",
    "\n",
    "df['word2vec_similarity'] = df.apply(lambda row: calculate_word2vec_similarity(row['question1'], row['question2']), axis=1)\n",
    "\n",
    "# Calculate word count\n",
    "df['word_count'] = df['preprocessed_text'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Calculate the number of the same words in the combined text column\n",
    "df['same_word_count'] = df['combined_text'].apply(lambda text: len(set(nltk.word_tokenize(text.lower()))) if text else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['preprocessed_text'].apply(lambda text: len(text.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_counts = df['word_count'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8      33193\n",
       "10     29985\n",
       "9      29723\n",
       "6      27327\n",
       "7      26191\n",
       "       ...  \n",
       "66         1\n",
       "128        1\n",
       "67         1\n",
       "73         1\n",
       "54         1\n",
       "Name: word_count, Length: 80, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125\n",
      "Precision: 0.6985743380855397\n",
      "Recall: 0.44545454545454544\n",
      "F1-Score: 0.5440126883425852\n",
      "Confusion Matrix:\n",
      "[[1082  148]\n",
      " [ 427  343]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125\n",
      "Precision: 0.6985743380855397\n",
      "Recall: 0.44545454545454544\n",
      "F1-Score: 0.5440126883425852\n",
      "Confusion Matrix:\n",
      "[[1082  148]\n",
      " [ 427  343]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "class_names = unique_labels(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=True, title='Normalized Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125\n",
      "0.6985743380855397\n",
      "0.44545454545454544\n",
      "0.5440126883425852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [     nan 0.618625      nan 0.618625      nan 0.6555        nan 0.7125\n",
      "      nan 0.6945  ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#improve model accuracy and precision\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(logreg_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new logistic regression model with the best hyperparameters\n",
    "best_logreg_model = LogisticRegression(**best_params)\n",
    "\n",
    "# Fit the new model to the training data\n",
    "best_logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print (accuracy)\n",
    "print (precision)\n",
    "print (recall)\n",
    "print (f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125\n",
      "Precision: 0.6985743380855397\n",
      "Recall: 0.44545454545454544\n",
      "F1-Score: 0.5440126883425852\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.88      0.79      1230\n",
      "           1       0.70      0.45      0.54       770\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.71      0.66      0.67      2000\n",
      "weighted avg       0.71      0.71      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg_model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(logreg_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new logistic regression model with the best hyperparameters\n",
    "best_logreg_model = LogisticRegression(solver='lbfgs', **best_params)\n",
    "\n",
    "# Fit the new model to the training data\n",
    "best_logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.709\n",
      "Precision: 0.6530944625407166\n",
      "Recall: 0.5207792207792208\n",
      "F1-Score: 0.5794797687861272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#using XGBOOST\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.83      0.78      1230\n",
      "           1       0.65      0.52      0.58       770\n",
      "\n",
      "    accuracy                           0.71      2000\n",
      "   macro avg       0.69      0.67      0.68      2000\n",
      "weighted avg       0.70      0.71      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.707\n",
      "Precision: 0.7628571428571429\n",
      "Recall: 0.34675324675324676\n",
      "F1-Score: 0.4767857142857143\n"
     ]
    }
   ],
   "source": [
    "#using ensemble modelling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create individual models\n",
    "logreg_model = LogisticRegression()\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[('logreg', logreg_model), ('xgb', xgb_model)])\n",
    "\n",
    "# Fit the ensemble model to the training data\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('feature_selection', SelectKBest(chi2)),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000, 20000],  # Adjust the number of features as needed\n",
    "    'feature_selection__k': [100, 500, 1000],  # Adjust the number of top features as needed\n",
    "    'classification__n_estimators': [100, 200, 300],\n",
    "    'classification__max_depth': [None, 5, 10],\n",
    "    'classification__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Reduce the number of samples (if needed) and define the features and labels columns\n",
    "df_sampled = df.sample(n=10000, random_state=42)\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with the tuned hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the testing data\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000  # Number of top features to select\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_test = selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 5, 10],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 5, 10],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 5, 10],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6915\n",
      "Precision: 0.6335078534031413\n",
      "Recall: 0.4714285714285714\n",
      "F1 Score: 0.5405807892777363\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 18s 66ms/step - loss: 0.6097 - accuracy: 0.6700\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 0.4476 - accuracy: 0.7951\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.2955 - accuracy: 0.8748\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 0.1895 - accuracy: 0.9229\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 0.1266 - accuracy: 0.9495\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.0879 - accuracy: 0.9638\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 0.0643 - accuracy: 0.9749\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 0.0443 - accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.0290 - accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.0512 - accuracy: 0.9821\n",
      "63/63 [==============================] - 5s 20ms/step\n",
      "Accuracy: 0.6445\n",
      "Precision: 0.532741398446171\n",
      "Recall: 0.6233766233766234\n",
      "F1 Score: 0.5745062836624775\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max([len(seq) for seq in X_train_seq])\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(X_test_padded)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the models with new data\n",
    "#import the testing data\n",
    "\n",
    "df_test = pd.read_csv(r\"C:\\Users\\Tombra\\train.csv\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#cleaning the data\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download stopwords and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Convert non-string columns to strings\n",
    "df_test['question1'] = df_test['question1'].astype(str)\n",
    "df_test['question2'] = df_test['question2'].astype(str)\n",
    "\n",
    "# Combine two text columns into a single column\n",
    "df_test['combined_text'] = df_test['question1'] + df_test['question2']\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stopwords cleaning and punctuation removal\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Normalization (optional, can be skipped depending on the use case)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing function to the combined text column\n",
    "df_test['preprocessed_text'] = df_test['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Example usage: Create a bag-of-words representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_test['preprocessed_text'])\n",
    "\n",
    "# X now contains the vectorized representation of the preprocessed text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = df_test['combined_text']\n",
    "labels_test = df_test['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Calculate TF-IDF\n",
    "# Set the parameters\n",
    "max_features = 2500\n",
    "min_df = 7\n",
    "max_df = 0.8\n",
    "stop_words = nltk_stopwords.words('english')  # Use nltk_stopwords instead of stopwords\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer to the features data\n",
    "tfidf_vectorizer.fit(features_test)\n",
    "\n",
    "# Transform the features using the fitted vectorizer\n",
    "vectorized_features_test= tfidf_vectorizer.transform(features_test)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model_test = gensim.models.Word2Vec.load(r\"C:\\Users\\Tombra\\word2vec\\model.bin\")\n",
    "\n",
    "def calculate_word2vec_similarity(text1, text2):\n",
    "    tokens1 = nltk.word_tokenize(text1)\n",
    "    tokens2 = nltk.word_tokenize(text2)\n",
    "    # Remove stopwords and punctuation\n",
    "    stopwords = set(nltk_stopwords.words('english'))\n",
    "    tokens1 = [token.lower() for token in tokens1 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    tokens2 = [token.lower() for token in tokens2 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Check if any of the lists is empty\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate similarity using Word2Vec model\n",
    "    similarity = model_test.wv.n_similarity(tokens1, tokens2)\n",
    "    return similarity\n",
    "\n",
    "df_test['word2vec_similarity'] = df_test.apply(lambda row: calculate_word2vec_similarity(row['question1'], row['question2']), axis=1)\n",
    "\n",
    "# Calculate word count\n",
    "df_test['word_count'] = df_test['preprocessed_text'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Calculate the number of the same words in the combined text column\n",
    "df_test['same_word_count'] = df_test['combined_text'].apply(lambda text: len(set(nltk.word_tokenize(text.lower()))) if text else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       int64\n",
       "qid1                     int64\n",
       "qid2                     int64\n",
       "question1               object\n",
       "question2               object\n",
       "is_duplicate             int32\n",
       "combined_text           object\n",
       "preprocessed_text       object\n",
       "word2vec_similarity    float64\n",
       "word_count               int64\n",
       "same_word_count          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 50s 19ms/step\n",
      "Accuracy (Test): 0.6527636886928212\n",
      "Precision (Test): 0.5149125630297179\n",
      "Recall (Test): 0.6284124386252046\n",
      "F1-Score (Test): 0.5660288350974438\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the new test data\n",
    "X_test = df_test['combined_text']\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Predict with the LSTM model\n",
    "y_pred_probs_test = model.predict(X_test_padded)\n",
    "y_pred_test = (y_pred_probs_test > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics on the new test dataset\n",
    "accuracy_test = accuracy_score(df_test['is_duplicate'], y_pred_test)\n",
    "precision_test = precision_score(df_test['is_duplicate'], y_pred_test)\n",
    "recall_test = recall_score(df_test['is_duplicate'], y_pred_test)\n",
    "f1_test = f1_score(df_test['is_duplicate'], y_pred_test)\n",
    "\n",
    "print(\"Accuracy (Test):\", accuracy_test)\n",
    "print(\"Precision (Test):\", precision_test)\n",
    "print(\"Recall (Test):\", recall_test)\n",
    "print(\"F1-Score (Test):\", f1_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
