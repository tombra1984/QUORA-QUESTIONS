{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Duplicate Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 100 million people visit Quora every month, so it's no surprise that many people ask similar (or the same) questions. Various questions with the same intent can cause people to spend extra time searching for the best answer to their question, and results in members answering multiple versions of the same question. Quora uses random forest to identify duplicated questions to provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n",
    "Follow the steps outlined below to build the appropriate classifier model. \n",
    "\n",
    "\n",
    "Steps:\n",
    "- Download data\n",
    "- Exploration\n",
    "- Cleaning\n",
    "- Feature Engineering\n",
    "- Modeling\n",
    "\n",
    "By the end of this project you should have **a presentation that describes the model you built** and its **performance**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Tombra\\train.csv\\train - Copy.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "There is no designated test.csv file. The train.csv file is the entire dataset. Part of the data in the train.csv file should be set aside to act as the final testing data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319512, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning\n",
    "\n",
    "- Tokenization\n",
    "- Stopwords cleaning\n",
    "- Removing punctuation\n",
    "- Normalizing\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download stopwords and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Convert non-string columns to strings\n",
    "df['question1'] = df['question1'].astype(str)\n",
    "df['question2'] = df['question2'].astype(str)\n",
    "\n",
    "# Combine two text columns into a single column\n",
    "df['combined_text'] = df['question1'] + df['question2']\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stopwords cleaning and punctuation removal\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Normalization (optional, can be skipped depending on the use case)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing function to the combined text column\n",
    "df['preprocessed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Example usage: Create a bag-of-words representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['preprocessed_text'])\n",
    "\n",
    "# X now contains the vectorized representation of the preprocessed text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- tf-idf\n",
    "- word2vec\n",
    "- word count\n",
    "- number of the same words in both questions\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize the text data\n",
    "sentences = [nltk.word_tokenize(text) for text in df['combined_text']]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(r\"C:\\Users\\Tombra\\word2vec\\model.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df['combined_text']\n",
    "labels = df['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Calculate TF-IDF\n",
    "# Set the parameters\n",
    "max_features = 2500\n",
    "min_df = 7\n",
    "max_df = 0.8\n",
    "stop_words = nltk_stopwords.words('english')  # Use nltk_stopwords instead of stopwords\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer to the features data\n",
    "tfidf_vectorizer.fit(features)\n",
    "\n",
    "# Transform the features using the fitted vectorizer\n",
    "vectorized_features = tfidf_vectorizer.transform(features)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model = gensim.models.Word2Vec.load(r\"C:\\Users\\Tombra\\word2vec\\model.bin\")\n",
    "\n",
    "def calculate_word2vec_similarity(text1, text2):\n",
    "    tokens1 = nltk.word_tokenize(text1)\n",
    "    tokens2 = nltk.word_tokenize(text2)\n",
    "    # Remove stopwords and punctuation\n",
    "    stopwords = set(nltk_stopwords.words('english'))\n",
    "    tokens1 = [token.lower() for token in tokens1 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    tokens2 = [token.lower() for token in tokens2 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Check if any of the lists is empty\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate similarity using Word2Vec model\n",
    "    similarity = model.wv.n_similarity(tokens1, tokens2)\n",
    "    return similarity\n",
    "\n",
    "df['word2vec_similarity'] = df.apply(lambda row: calculate_word2vec_similarity(row['question1'], row['question2']), axis=1)\n",
    "\n",
    "# Calculate word count\n",
    "df['word_count'] = df['preprocessed_text'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Calculate the number of the same words in the combined text column\n",
    "df['same_word_count'] = df['combined_text'].apply(lambda text: len(set(nltk.word_tokenize(text.lower()))) if text else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['preprocessed_text'].apply(lambda text: len(text.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_counts = df['word_count'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8      33193\n",
       "10     29985\n",
       "9      29723\n",
       "6      27327\n",
       "7      26191\n",
       "       ...  \n",
       "66         1\n",
       "128        1\n",
       "67         1\n",
       "73         1\n",
       "54         1\n",
       "Name: word_count, Length: 80, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Access the calculated metrics\n",
    "tfidf_matrix  # TF-IDF matrix\n",
    "df['word2vec_similarity']  # Word2Vec similarity column\n",
    "df['word_count']  # Word count column\n",
    "df['same_word_count']  # Number of same words column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Different modeling techniques can be used:\n",
    "\n",
    "- logistic regression\n",
    "- XGBoost\n",
    "- LSTMs\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125\n",
      "Precision: 0.6985743380855397\n",
      "Recall: 0.44545454545454544\n",
      "F1-Score: 0.5440126883425852\n",
      "Confusion Matrix:\n",
      "[[1082  148]\n",
      " [ 427  343]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7125\n",
      "Precision: 0.6985743380855397\n",
      "Recall: 0.44545454545454544\n",
      "F1-Score: 0.5440126883425852\n",
      "Confusion Matrix:\n",
      "[[1082  148]\n",
      " [ 427  343]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "class_names = unique_labels(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(conf_mat, classes=class_names, normalize=True, title='Normalized Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125\n",
      "0.6985743380855397\n",
      "0.44545454545454544\n",
      "0.5440126883425852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "25 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [     nan 0.618625      nan 0.618625      nan 0.6555        nan 0.7125\n",
      "      nan 0.6945  ]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#improve model accuracy and precision\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(logreg_model, param_grid, cv=5)\n",
    "\n",
    "# Fit the GridSearchCV object on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create a new logistic regression model with the best hyperparameters\n",
    "best_logreg_model = LogisticRegression(**best_params)\n",
    "\n",
    "# Fit the new model to the training data\n",
    "best_logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_logreg_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print (accuracy)\n",
    "print (precision)\n",
    "print (recall)\n",
    "print (f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.709\n",
      "Precision: 0.6530944625407166\n",
      "Recall: 0.5207792207792208\n",
      "F1-Score: 0.5794797687861272\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#using XGBOOST\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.707\n",
      "Precision: 0.7628571428571429\n",
      "Recall: 0.34675324675324676\n",
      "F1-Score: 0.4767857142857143\n"
     ]
    }
   ],
   "source": [
    "#using ensemble modelling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# Create individual models\n",
    "logreg_model = LogisticRegression()\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[('logreg', logreg_model), ('xgb', xgb_model)])\n",
    "\n",
    "# Fit the ensemble model to the training data\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = ensemble_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Perform grid search for hyperparameter tuning\u001b[39;00m\n\u001b[0;32m     34\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     37\u001b[0m \u001b[39m# Get the best model with the tuned hyperparameters\u001b[39;00m\n\u001b[0;32m     38\u001b[0m best_model \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1052\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('feature_selection', SelectKBest(chi2)),\n",
    "    ('classification', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000, 20000],  # Adjust the number of features as needed\n",
    "    'feature_selection__k': [100, 500, 1000],  # Adjust the number of top features as needed\n",
    "    'classification__n_estimators': [100, 200, 300],\n",
    "    'classification__max_depth': [None, 5, 10],\n",
    "    'classification__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Reduce the number of samples (if needed) and define the features and labels columns\n",
    "df_sampled = df.sample(n=10000, random_state=42)\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model with the tuned hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the testing data\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1000  # Number of top features to select\n",
    "selector = SelectKBest(chi2, k=k)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_test = selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 5, 10],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={&#x27;max_depth&#x27;: [None, 5, 10],\n",
       "                         &#x27;min_samples_split&#x27;: [2, 5, 10],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [None, 5, 10],\n",
       "                         'min_samples_split': [2, 5, 10],\n",
       "                         'n_estimators': [100, 200, 300]})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_rf_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6915\n",
      "Precision: 0.6335078534031413\n",
      "Recall: 0.4714285714285714\n",
      "F1 Score: 0.5405807892777363\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 18s 66ms/step - loss: 0.6097 - accuracy: 0.6700\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 0.4476 - accuracy: 0.7951\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 18s 71ms/step - loss: 0.2955 - accuracy: 0.8748\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 18s 74ms/step - loss: 0.1895 - accuracy: 0.9229\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 20s 80ms/step - loss: 0.1266 - accuracy: 0.9495\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.0879 - accuracy: 0.9638\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 19s 78ms/step - loss: 0.0643 - accuracy: 0.9749\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 20s 79ms/step - loss: 0.0443 - accuracy: 0.9833\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.0290 - accuracy: 0.9895\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.0512 - accuracy: 0.9821\n",
      "63/63 [==============================] - 5s 20ms/step\n",
      "Accuracy: 0.6445\n",
      "Precision: 0.532741398446171\n",
      "Recall: 0.6233766233766234\n",
      "F1 Score: 0.5745062836624775\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Reduce the number of samples\n",
    "df_sampled = df.sample(n=10000, random_state=42)  # Adjust the number of samples as needed\n",
    "\n",
    "# Define the features and labels columns\n",
    "features = 'combined_text'\n",
    "labels = 'is_duplicate'\n",
    "\n",
    "# Split the sampled data into features and labels\n",
    "X = df_sampled[features]\n",
    "y = df_sampled[labels]\n",
    "\n",
    "# Split the features and labels into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = max([len(seq) for seq in X_train_seq])\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_probs = model.predict(X_test_padded)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the models with new data\n",
    "#import the testing data\n",
    "\n",
    "df_test = pd.read_csv(r\"C:\\Users\\Tombra\\train.csv\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#cleaning the data\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Download stopwords and initialize stemmer\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Convert non-string columns to strings\n",
    "df_test['question1'] = df_test['question1'].astype(str)\n",
    "df_test['question2'] = df_test['question2'].astype(str)\n",
    "\n",
    "# Combine two text columns into a single column\n",
    "df_test['combined_text'] = df_test['question1'] + df_test['question2']\n",
    "\n",
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Stopwords cleaning and punctuation removal\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Normalization (optional, can be skipped depending on the use case)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a single string\n",
    "    preprocessed_text = ' '.join(stemmed_tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Apply preprocessing function to the combined text column\n",
    "df_test['preprocessed_text'] = df_test['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Example usage: Create a bag-of-words representation using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df_test['preprocessed_text'])\n",
    "\n",
    "# X now contains the vectorized representation of the preprocessed text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test = df_test['combined_text']\n",
    "labels_test = df_test['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tombra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Calculate TF-IDF\n",
    "# Set the parameters\n",
    "max_features = 2500\n",
    "min_df = 7\n",
    "max_df = 0.8\n",
    "stop_words = nltk_stopwords.words('english')  # Use nltk_stopwords instead of stopwords\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, min_df=min_df, max_df=max_df, stop_words=stop_words)\n",
    "\n",
    "# Fit the vectorizer to the features data\n",
    "tfidf_vectorizer.fit(features_test)\n",
    "\n",
    "# Transform the features using the fitted vectorizer\n",
    "vectorized_features_test= tfidf_vectorizer.transform(features_test)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "model_test = gensim.models.Word2Vec.load(r\"C:\\Users\\Tombra\\word2vec\\model.bin\")\n",
    "\n",
    "def calculate_word2vec_similarity(text1, text2):\n",
    "    tokens1 = nltk.word_tokenize(text1)\n",
    "    tokens2 = nltk.word_tokenize(text2)\n",
    "    # Remove stopwords and punctuation\n",
    "    stopwords = set(nltk_stopwords.words('english'))\n",
    "    tokens1 = [token.lower() for token in tokens1 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    tokens2 = [token.lower() for token in tokens2 if token.lower() not in stopwords and token not in string.punctuation]\n",
    "    \n",
    "    # Check if any of the lists is empty\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate similarity using Word2Vec model\n",
    "    similarity = model_test.wv.n_similarity(tokens1, tokens2)\n",
    "    return similarity\n",
    "\n",
    "df_test['word2vec_similarity'] = df_test.apply(lambda row: calculate_word2vec_similarity(row['question1'], row['question2']), axis=1)\n",
    "\n",
    "# Calculate word count\n",
    "df_test['word_count'] = df_test['preprocessed_text'].apply(lambda text: len(text.split()))\n",
    "\n",
    "# Calculate the number of the same words in the combined text column\n",
    "df_test['same_word_count'] = df_test['combined_text'].apply(lambda text: len(set(nltk.word_tokenize(text.lower()))) if text else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 13409, got 2500",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m X_test \u001b[39m=\u001b[39m vectorized_features_test \n\u001b[0;32m     11\u001b[0m \u001b[39m# Make predictions with the XGBoost model\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m y_pred_test \u001b[39m=\u001b[39m xgb_model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Calculate evaluation metrics on the new dataset\u001b[39;00m\n\u001b[0;32m     15\u001b[0m accuracy_test \u001b[39m=\u001b[39m accuracy_score(df_test[\u001b[39m'\u001b[39m\u001b[39mis_duplicate\u001b[39m\u001b[39m'\u001b[39m], y_pred_test)\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1525\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[0;32m   1516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1517\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m     iteration_range: Optional[Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1523\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1524\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(verbosity\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbosity):\n\u001b[1;32m-> 1525\u001b[0m         class_probs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m   1526\u001b[0m             X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1527\u001b[0m             output_margin\u001b[39m=\u001b[39;49moutput_margin,\n\u001b[0;32m   1528\u001b[0m             ntree_limit\u001b[39m=\u001b[39;49mntree_limit,\n\u001b[0;32m   1529\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1530\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1531\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1532\u001b[0m         )\n\u001b[0;32m   1533\u001b[0m         \u001b[39mif\u001b[39;00m output_margin:\n\u001b[0;32m   1534\u001b[0m             \u001b[39m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m             \u001b[39mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1114\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1113\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1114\u001b[0m         predts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_booster()\u001b[39m.\u001b[39;49minplace_predict(\n\u001b[0;32m   1115\u001b[0m             data\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m   1116\u001b[0m             iteration_range\u001b[39m=\u001b[39;49miteration_range,\n\u001b[0;32m   1117\u001b[0m             predict_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmargin\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m output_margin \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mvalue\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1118\u001b[0m             missing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmissing,\n\u001b[0;32m   1119\u001b[0m             base_margin\u001b[39m=\u001b[39;49mbase_margin,\n\u001b[0;32m   1120\u001b[0m             validate_features\u001b[39m=\u001b[39;49mvalidate_features,\n\u001b[0;32m   1121\u001b[0m         )\n\u001b[0;32m   1122\u001b[0m         \u001b[39mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1123\u001b[0m             \u001b[39mimport\u001b[39;00m \u001b[39mcupy\u001b[39;00m  \u001b[39m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tombra\\anaconda3\\lib\\site-packages\\xgboost\\core.py:2269\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2265\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2266\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2267\u001b[0m         )\n\u001b[0;32m   2268\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features() \u001b[39m!=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m-> 2269\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2270\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature shape mismatch, expected: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features()\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2271\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00mdata\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2272\u001b[0m         )\n\u001b[0;32m   2274\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m   2275\u001b[0m     _array_interface,\n\u001b[0;32m   2276\u001b[0m     _is_cudf_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2279\u001b[0m     _transform_pandas_df,\n\u001b[0;32m   2280\u001b[0m )\n\u001b[0;32m   2282\u001b[0m enable_categorical \u001b[39m=\u001b[39m _has_categorical(\u001b[39mself\u001b[39m, data)\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 13409, got 2500"
     ]
    }
   ],
   "source": [
    "# Convert 'is_duplicate' column to int data type\n",
    "df_test['is_duplicate'] = df_test['is_duplicate'].astype(int)\n",
    "\n",
    "\n",
    "# Fit the XGBoost model to the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Transform the new dataset using the fitted vectorizer\n",
    "X_test = vectorized_features_test \n",
    "\n",
    "# Make predictions with the XGBoost model\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics on the new dataset\n",
    "accuracy_test = accuracy_score(df_test['is_duplicate'], y_pred_test)\n",
    "precision_test = precision_score(df_test['is_duplicate'], y_pred_test)\n",
    "recall_test = recall_score(df_test['is_duplicate'], y_pred_test)\n",
    "f1_test = f1_score(df_test['is_duplicate'], y_pred_test)\n",
    "\n",
    "print(\"Accuracy (Test):\", accuracy_test)\n",
    "print(\"Precision (Test):\", precision_test)\n",
    "print(\"Recall (Test):\", recall_test)\n",
    "print(\"F1-Score (Test):\", f1_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       int64\n",
       "qid1                     int64\n",
       "qid2                     int64\n",
       "question1               object\n",
       "question2               object\n",
       "is_duplicate             int32\n",
       "combined_text           object\n",
       "preprocessed_text       object\n",
       "word2vec_similarity    float64\n",
       "word_count               int64\n",
       "same_word_count          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 50s 19ms/step\n",
      "Accuracy (Test): 0.6527636886928212\n",
      "Precision (Test): 0.5149125630297179\n",
      "Recall (Test): 0.6284124386252046\n",
      "F1-Score (Test): 0.5660288350974438\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the new test data\n",
    "X_test = df_test['combined_text']\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Predict with the LSTM model\n",
    "y_pred_probs_test = model.predict(X_test_padded)\n",
    "y_pred_test = (y_pred_probs_test > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics on the new test dataset\n",
    "accuracy_test = accuracy_score(df_test['is_duplicate'], y_pred_test)\n",
    "precision_test = precision_score(df_test['is_duplicate'], y_pred_test)\n",
    "recall_test = recall_score(df_test['is_duplicate'], y_pred_test)\n",
    "f1_test = f1_score(df_test['is_duplicate'], y_pred_test)\n",
    "\n",
    "print(\"Accuracy (Test):\", accuracy_test)\n",
    "print(\"Precision (Test):\", precision_test)\n",
    "print(\"Recall (Test):\", recall_test)\n",
    "print(\"F1-Score (Test):\", f1_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
